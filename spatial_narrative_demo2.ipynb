{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnatiusEzeani/spatial_narratives_workshop/blob/main/spatial_narrative_demo2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUojVOiIhoEv"
      },
      "source": [
        "# **Extracting Spatial Entities from text (2)**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ0kCbYUh4gu"
      },
      "source": [
        "## Task Description:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcfL-sEdGCIm"
      },
      "source": [
        "![](https://raw.githubusercontent.com/IgnatiusEzeani/spatial_narratives_workshop/main/img/from_penrith_both.png)\n",
        "\n",
        "Assuming we know nothing about the geography of the place(s) described by the corpus, what can we learn about it. In particular:\n",
        "* **What places are there?** These can be:\n",
        " * `Toponyms` (*Keswick*, *Pooley Bridge*, *the River Lowther*)\n",
        " * `Geographical features` (*the town*, *a hill*, *the road*)\n",
        " * `Locative adverbs` (*above*, *north-of*, *eastwards*, *here*, *there*)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition and Semantic Tagging\n",
        "Previously, we applied the a rule based method to spatial elements extraction from text in **Demo 1**.\n",
        "\n",
        "There were a few limitations with the rule-based method.\n",
        "* It requires a complete list of entities.\n",
        "* Rules need to be provided for all possible scenarios:\n",
        "  - e.g. spelling errors, variations in capitalizations, inflections etc.\n",
        "  - Over-lapping instances ('Eamont' vs 'Eamont Bridge')\n",
        "* Difficult to extract references to time and date as well as sentiments and emotions.\n",
        "* Does not generalize well with other corpora\n",
        "\n",
        "In this demo, we will explore the option of adapting named entity recognition and semantic tagging systems. "
      ],
      "metadata": {
        "id": "lZQex_MqCF6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Downloading the workshop materials**\n",
        "Let's download (clone) the resources for the workshop from the [Spatial Narrative Workshop](https://github.com/IgnatiusEzeani/spatial_narratives_workshop)  GitHub repository."
      ],
      "metadata": {
        "id": "VQzZg_4jQ9Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SpaceTimeNarratives/demo.git"
      ],
      "metadata": {
        "id": "16hV3t2_K-Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/SpaceTimeNarratives/demo/main/img/file_structure.png\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "The `demo` directory contains an example file `example_text.txt`. Our aim is to read file and display the text as well as identify all the place names mentioned in the text.\n",
        "\n",
        "### Changing into the working directory\n",
        "Everything we need for this exercise can be found in the working directory (or folder) named *demo*. We will use the `os` (operating system) library which contains all the useful functions we may need to manage our folders and files programmatically. \n",
        "\n",
        "Here we use the `chdir()` (change directory) function to get into our working directory and list the contents of our directory using the `listdir()` \n",
        "\n",
        "Type (or copy and paste) the code below in the next cell and run.\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.chdir('demo/')\n",
        "os.listdir()\n",
        "```"
      ],
      "metadata": {
        "id": "-gHbsXiiSIGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type or paste the command below:\n",
        "import os\n",
        "os.chdir('demo/')\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "cSzKcVhYR-1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the `function.py` file \n",
        "We have wrapped up all the functions we created in the previous demo into a Python file `functions.py`. \n",
        "\n",
        "One of the functions depend on the `lemminflect` Python library that helps to generate lemmas and inflections, so we have to install it first."
      ],
      "metadata": {
        "id": "-KrCvbY9FZ_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lemminflect"
      ],
      "metadata": {
        "id": "NCTD-4cFFRTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run functions.py"
      ],
      "metadata": {
        "id": "IImMWCj9FA_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Extracting named entitities**\n",
        "The [spaCy NLP](https://spacy.io/) library provides a named entity recognizer that we can use. So we start by importing `spacy`"
      ],
      "metadata": {
        "id": "8wf3pdzkWFS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing `spaCy`\n",
        "We need to import the `spaCy` NLP pipeline\n",
        "and load the small version of the English model `en_core_web_sm` for tokenization, tagging, parsing and named entity recognition."
      ],
      "metadata": {
        "id": "lkJtHFM5OIsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "07HSvAlIHkD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "KGD-O_OHIsfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process `example.txt`\n",
        "Read the content of the `example.txt` file in the working folder into the `example_text` variable and pass the variable through the NLP pipeline to produce the `spacy_processed` document."
      ],
      "metadata": {
        "id": "KKTwhz36JauW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = open('example.txt').read()\n",
        "# example_text"
      ],
      "metadata": {
        "id": "vsZtSxJNJlM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_processed = nlp(example_text)"
      ],
      "metadata": {
        "id": "Xk2uum74MHWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see all the entities and tags the tagger has seen from `processed_text`..."
      ],
      "metadata": {
        "id": "WLXOuT5qM_Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in spacy_processed.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "metadata": {
        "id": "fzqp4BQiNKyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks quite interesting. We have more tags in the spacy NER tagger and, unsurprisingly, there is no `PLNAME` or `GEONOUN`. We can see that even without a list, it identified similar entities to the regular expression method, and even more (e.g. `King Arthur's Round Table` and `Lowther Castle`), although with different tags."
      ],
      "metadata": {
        "id": "CdnInQGvNyB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the entities\n",
        "Spacy has an inbuilt visualisation feature which we can call as below:\n",
        "\n",
        "```python\n",
        "from spacy import displacy\n",
        "HTML(displacy.render(spacy_processed, style=\"ent\"))\n",
        "```\n",
        "However, to have a bit of control over the visualization, we can use the `visualizer()` we built in the last demo. Therefore we need a function `extract_spacy_entities(spacy_processed)` to extract the spacy entities into a dictionary and build the list of tagged tokens for visualization.\n",
        "\n",
        "We can convert all tags refering to a place (e.g. `GPE`, `ORG` and `FAC`) to `PLNAME`. Also we have redefined the `BG_COLOR` variable in the `function.py` file to accommodate all possible spacy NER tags (See pages 21 & 22 of the [OntoNotes 5.0](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf) document for the descriptions of the NER tags) and others that we may need later.\n",
        "\n",
        "```python\n",
        "BG_COLOR = {\n",
        "    'PLNAME':'#feca74','GEONOUN': '#9cc9cc', 'GPE':'#feca74', 'CARDINAL':'#e4e7d2',\n",
        "    'FAC':'#9cc9cc','QUANTITY':'#e4e7d2','PERSON':'#aa9cfc', 'ORDINAL':'#e4e7d2',\n",
        "    'ORG':'#7aecec', 'NORP':'#d9fe74', 'LOC':'#9ac9f5', 'DATE':'#c7f5a9',\n",
        "    'PRODUCT':'#edf5a9', 'EVENT': '#e1a9f5','TIME':'#a9f5bc', 'WORK_OF_ART':'#e6c1d7',\n",
        "    'LAW':'#e6e6c1','LANGUAGE':'#c9bdc7', 'PERCENT':'#c9ebf5', 'MONEY':'#b3d6f2',\n",
        "    'EMOTION':'#f2ecd0', 'TIME-sem':'#d0e0f2', 'MOVEMENT':'#f2d0d0','no_tag':'#FFFFFF'\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "PL3ySp9PQkfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_spacy_entities(processed_text):\n",
        "  entities = {}\n",
        "  for ent in processed_text.ents:\n",
        "    tag='PLNAME' if ent.label_ in ['GPE', 'ORG', 'FAC', 'LOC'] else ent.label_\n",
        "    entities[ent.start_char] = ent.text, tag\n",
        "  return OrderedDict(sorted(entities.items()))\n",
        "\n",
        "spacy_entities = extract_spacy_entities(spacy_processed)\n",
        "visualize(get_tagged_list(example_text, spacy_entities))"
      ],
      "metadata": {
        "id": "voXEM2hssahQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining the rule-based and `spaCy` methods\n",
        "As shown above, some of the placenames are tagged as `PERSON`. However, the regular expression rules method used previously was able to identify them as placenames using the Lake District gazetteer.\n",
        "\n",
        "So using the `merge_entities()` function, we can combine the outputs from both methods in a way that the output of the regular expression overrides that of spacy NER where there is conflict."
      ],
      "metadata": {
        "id": "8mh42MbbperQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read LD placenames into a list\n",
        "ld_place_names = [name.strip() for name in open('LD_placenames.txt').readlines()]\n",
        "\n",
        "regex_entities = extract_entities(example_text, ld_place_names)"
      ],
      "metadata": {
        "id": "3ZLKHuqUmhDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(get_tagged_list(example_text, merge_entities(regex_entities, spacy_entities)))"
      ],
      "metadata": {
        "id": "Ol_wIppZnEpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Including Geo nouns and locative adverbs\n",
        "Great so far! We can also include the geo nouns and the locative adverbs in our extracted entity types using their respective lists.\n",
        "\n",
        "Remember that `merge_entities()` accepts two extracted entities arguments and the order matters because the first overrides the other when there is a conflict."
      ],
      "metadata": {
        "id": "ORrOprrIwS_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract geo nouns...\n",
        "geonouns = get_inflections([noun.strip() for noun in open('geo_feature_nouns.txt').readlines()])\n",
        "extracted_geonouns = extract_entities(example_text, geonouns, tag='GEONOUN')\n",
        "\n",
        "# Extract locative adverbs \n",
        "loc_advs = [adv.split()[0] for adv in open('locativeAdverbs.txt').readlines()]\n",
        "extracted_locadvs = extract_entities(example_text, geonouns,  tag='LOCADV')\n",
        "\n",
        "# Merger the entities in the other below\n",
        "merged_entities = merge_entities(regex_entities,\n",
        "                    merge_entities(spacy_entities,\n",
        "                       merge_entities(extracted_geonouns, extracted_locadvs)))\n",
        "# Visualize the merged entities.\n",
        "visualize(get_tagged_list(example_text, merged_entities))"
      ],
      "metadata": {
        "id": "s7WnB0wunEmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So far...\n",
        "...this is going great ðŸ˜Š. \n",
        "\n",
        "##### **Rule based method**\n",
        "With the rule based method, we can identify, extract and merge any number of entity types (placenames, geo nouns, locative adverbs) as long as we have a list of named elements in that class. But it is inefficient to create an exhaustive list that can generalize across different writings. \n",
        "\n",
        "##### **Named entity recognizer**\n",
        "Fortunately, we can use a named entity recognizer identify interesting entities even without a list of elements. Although the NER model was not trained for our case study, we can adapt the relevant tags from the model output that correspond to our categories e.g. converting `[GPE, ORG, FAC, LOC]` to `PLNAME`."
      ],
      "metadata": {
        "id": "E4FaSHapghyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Extracting semantic entitities**\n",
        "\n",
        "We may also want extract spatial elements that indicate the movements, emotion or a sense of time. The previous methods are not capable of identifying references to movements and emotions.\n",
        "\n",
        "Altough the NER method can detect time and date references (e.g. `August`, `daily`, `the year 1635`), it is not able to pick expressions like `retrospective`, `for some time`, `never ending`, `prolonged` etc., as references to time.\n",
        "\n",
        "We will use **P**ython **M**ultilingual **U**crel **S**emantic **A**nalysis **S**ystem ([PyMUSAS]( https://ucrel.github.io/pymusas/)), which is a rule based token and Multi Word Expression semantic tagger that uses the [Ucrel Semantic Analysis System (USAS)](https://ucrel.lancs.ac.uk/usas/ and runs on the spaCy pipeline.\n",
        "\n",
        "The USAS tagset three highlevel tags:\n",
        "* `E` - Emotion \n",
        "* `M` - Movement, location, travel and transport\n",
        "* `T` - Time\n"
      ],
      "metadata": {
        "id": "RJ0fCTEeav5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up `PyMUSAS`\n",
        "\n"
      ],
      "metadata": {
        "id": "aS56gcNhwP0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract all entities with semtagger\n",
        "def extract_entities_with_semtagger(tokens, index_list, tag):\n",
        "  entityPosLen={}\n",
        "  for i in index_list:\n",
        "    start_char = 1+len(\" \".join(tokens[:i]))\n",
        "    entityPosLen[start_char] = (len(tokens[i]), tokens[i], tag)\n",
        "  return entityPosLen"
      ],
      "metadata": {
        "id": "YTyqOcAEOtqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the index list of a sem tag\n",
        "def get_sem_tagged(tag_type):\n",
        "  index_list = []\n",
        "  for i in range(len(output_doc)):\n",
        "    if output_doc[i]._.pymusas_tags[0].startswith(tag_type[0]):\n",
        "       index_list.append(i)\n",
        "  return index_list"
      ],
      "metadata": {
        "id": "P7J8f0BAnEjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sem_entities():\n",
        "  entities = {}\n",
        "  for tag_type in sem_tag_types:\n",
        "    tag_indices = [(i, token.text, tag_type) for i, token in enumerate(self.nlp_doc) if token._.pymusas_tags[0].startswith(tag_type[0])]\n",
        "    if tag_indices:\n",
        "      for i, token, tag in combine_multi_tokens(tag_indices):\n",
        "        start_char = 1+len(\" \".join(self.tokens[:i]))\n",
        "        entities[start_char] = (len(token), token, tag)\n",
        "  return collections.OrderedDict(sorted(entities.items()))"
      ],
      "metadata": {
        "id": "JyP5EAhznEgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_types = ['EMOTION', 'MOVEMENT', 'TIME-Sem']\n",
        "semtagger_entities={}\n",
        "for tag_type in tag_types:\n",
        "  tag_entities = extract_entities_with_semtagger(text_tokens, get_sem_tagged(tag_type),tag_type) \n",
        "  semtagger_entities = {**semtagger_entities, **tag_entities}\n",
        "semtagger_entities = collections.OrderedDict(sorted(semtagger_entities.items()))\n",
        "\n",
        "IPython.display.HTML(\n",
        "    generate_html(get_token_tags(text, semtagger_entities)))"
      ],
      "metadata": {
        "id": "oArWtmlKnEdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VoW_9LZQnEa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6xWQgItnEW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzJQtlaEnETq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extracting placenames\n",
        "Here we think about a way to extract a known place name (e.g. `Penrith`) from the text. We will use the Python library `re` (regular expression) to build search patterns to look for in the text.\n",
        "\n",
        "So let's say we defining called function (or method) called `extract_placename(text, placename)` such that we can give it some *text* and a list of *placenames* and it returns all occurences of each of the placename in the text.\n",
        "\n",
        "The code below defines the `extract_placename(text, placename)` function.\n"
      ],
      "metadata": {
        "id": "LCQBR8lJTAii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_placenames(text, placenames):\n",
        "  sorted(set(placenames), key=lambda x:len(x), reverse=True)\n",
        "  extracted_placenames = {}\n",
        "  for name in placenames:\n",
        "    for match in re.finditer(f'{name}[\\.,\\s\\n;:]', text):\n",
        "      extracted_placenames[match.start()]=text[match.start():match.end()-1]\n",
        "  return {i:extracted_placenames[i] for i in sorted(extracted_placenames.keys())}"
      ],
      "metadata": {
        "id": "-w51uoIkdtdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "place_names = ['Carleton Hall', 'Dunmallet', 'Eamont', 'Eamont Bridge', \n",
        "               'Hallen Fell',  'Penrith', 'Pooley Bridge', 'Shap', 'ULLESWATER',\n",
        "               'Ulleswater']\n",
        "\n",
        "extracted_place_names = extract_placenames(example_text, place_names)\n",
        "extracted_place_names"
      ],
      "metadata": {
        "id": "HQ4dmJwUgoyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above output (`{5: 'Penrith', 31: 'Pooley Bridge', ...}`) is a dictionary with the `start: placename` entries where `start` is the starting character index (or position) of the `placename` in the order they are found in text. \n",
        "\n",
        "For example `Penrith` was the first placename found and the started from character index 5. `Pooley Bridge` appeared thrice in the text with starting indexes `31`, `450` and `856`. By the way the first character is in position `0` (not `1`).\n",
        "\n",
        "You may observe that it got all occurences of the placenames on our list. But there are other placenames in the text such as `King Arthur's Round Table`, `Lowther Castle`, `Martindale`, `Mayborough`, `Patterdale`. We will come back to this later.\n",
        "\n"
      ],
      "metadata": {
        "id": "O07te8A0ehBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Visualizing the outputs**\n",
        "---\n",
        "It is often a good idea to present a graphic representation of our outputs for better visualization and understanding of how our process works.\n",
        "\n",
        "Using the `HTML` function inside the `IPython` library's `display` package, we define functions that display the HTML format of the visualisation of the extracted place names in the text."
      ],
      "metadata": {
        "id": "mu3VPj2cqYid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Visualizing the plain text**\n",
        "\n",
        "Visualizing the untagged example text is easy. We simply pass the `example_text` variable to the I"
      ],
      "metadata": {
        "id": "-eKIFfzsaXRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML(example_text)"
      ],
      "metadata": {
        "id": "fw3i7Od_sH1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Visualizing the extracted place names**\n",
        "This is a little more challenging but will follow the same principle. Having extracted the place names, we can define functions that will help us 'mark-up' or highlight the extracted place names from the plain text based on their starting index positions and spans so we can visualize it in HTML format.\n",
        "\n",
        "Let's call the first function `get_tagged_list()`. It will parse the text with dictionary of extracted place names and identify spans that will be tagged as place names in the text. Its output is a list of tuples containing text spans and tags (either `PL-NAME` or `None`)"
      ],
      "metadata": {
        "id": "tgbwfPbh4TWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract all known place name in a list\n",
        "def get_tagged_list(text, ext_pl_names):\n",
        "  begin, tokens_tags = 0, []\n",
        "  for start, plname in ext_pl_names.items():\n",
        "    length, ent, tag = len(plname), plname, 'PLNAME'\n",
        "    if begin <= start:\n",
        "      tokens_tags.append((text[begin:start], None))\n",
        "      tokens_tags.append((text[start:start+length], tag))\n",
        "      begin = start+length\n",
        "  tokens_tags.append((text[begin:], None)) #add the last untagged chunk\n",
        "  return tokens_tags\n",
        "\n",
        "# get_tagged_list(example_text, extracted_place_names)"
      ],
      "metadata": {
        "id": "Wb0VTn717w7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second function `mark_up`, which takes a `token` (actually a span of characters) and a tag (i.e. `PL-NAME` for place name) basically marks up or highlights any piece of text with a given background colour in HTML format."
      ],
      "metadata": {
        "id": "MOHEigcrFvFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mark_up(token, tag=None):\n",
        "  if tag:\n",
        "    begin_bkgr = f'<bgr class=\"entity\" style=\"background: #feca74 ; padding: 0.1em 0.1em; margin: 0 0.15em; border-radius: 0.23em;\">'\n",
        "    end_bkgr = '\\n</bgr>'\n",
        "    begin_span = '<span style=\"font-size: 0.8em; font-weight: bold; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">'\n",
        "    end_span = '\\n</span>'\n",
        "    return f\"{begin_bkgr}{token}{begin_span}{tag}{end_span}{end_bkgr}\"\n",
        "  return f\"{token}\"\n",
        "# HTML(mark_up('Penrith', tag='PLNAME'))"
      ],
      "metadata": {
        "id": "UXSRDRNhFP5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we piece everything together with the function `generate_html()` which does exactly that by marking up the output of the `get_tagged_list()` with the `mark_up()` function."
      ],
      "metadata": {
        "id": "glVyOK7IIoxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate html formatted text \n",
        "def visualize(token_tag_list):\n",
        "  start_div = f'<div class=\"entities\" style=\"line-height: 2.0; direction: ltr\">'\n",
        "  end_div = '\\n</div>'\n",
        "  html = start_div\n",
        "  for token, tag in token_tag_list:\n",
        "    html += mark_up(token,tag)\n",
        "  html += end_div\n",
        "  return HTML(html)\n",
        "\n",
        "visualize(get_tagged_list(example_text, extracted_place_names))"
      ],
      "metadata": {
        "id": "UWpp-0NoFvrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Extracting with a gazetteer**\n",
        "Our previous examples so far is only able to extract and visualise a few place names. Obviously, for a chance to be able to extract all the place names in the text, we will need a more comprehensive list.\n",
        "\n",
        "So for this task, we will apply the techniques and processes defined above with a list of the Lake District place names from the gazetteer created by [Source]() to identify and extract mentions of the place names in the same text."
      ],
      "metadata": {
        "id": "3_g1_U1Cn_u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read LD placenames into a list\n",
        "ld_place_names = [name.strip() for name in open('LD_placenames.txt').readlines()]\n",
        "\n",
        "# Extract place names from the same example text \n",
        "extracted_place_names = extract_placenames(example_text, ld_place_names)\n",
        "\n",
        "# Get list of tagged entities (or placenames) and their tags\n",
        "tagged_list = get_tagged_list(example_text, extracted_place_names)\n",
        "\n",
        "visualize(tagged_list)"
      ],
      "metadata": {
        "id": "orzBT4Yhg9R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so far our method works quite well if we have what we are looking for in the list exactly as it appears in the text. Otherwise, it may wobble a bit. \n",
        "\n",
        "A typical example in the text above <mark>Lowther</mark> and <mark>Castle</mark> separately marked instead of <mark>Lowther Castle</mark>."
      ],
      "metadata": {
        "id": "CYUW-Zq-87-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Extracting geographical feature nouns with list**\n",
        "To extract geographical features from a list of feature nouns (e.g. `castle`, `ridge`, `forest`, `village`, `river` etc), we will apply the same method as placenames.\n",
        "\n",
        "We will modify the `extract_placenames()` function to be more generic for all entity classes i.e. `extract_entities()`.\n",
        "\n",
        "Also, to enable us apply a new tag `GEONOUN`, let's modify the `get_tagged_list()` function to accept the tag parameter that defaults to the `PLNAME` but supports other tags."
      ],
      "metadata": {
        "id": "b4RGgDoawe91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(text, ent_list):\n",
        "  sorted(set(ent_list), key=lambda x:len(x), reverse=True)\n",
        "  extracted_entities = {}\n",
        "  for ent in ent_list:\n",
        "    for match in re.finditer(f' {ent}[\\.,\\s\\n;:]', text):\n",
        "      extracted_entities[match.start()+1]=text[match.start()+1:match.end()-1]\n",
        "  return {i:extracted_entities[i] for i in sorted(extracted_entities.keys())}\n",
        "\n",
        "extract_entities(example_text, place_names)"
      ],
      "metadata": {
        "id": "egwljeRaCTwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Include `tag` in the `get_tagged_list()` parameters to enable the use of other tags `tag='PLNAME'` defaults to placenames but explicit passing of other tags (e.g. `GEONOUNS`) will override it."
      ],
      "metadata": {
        "id": "X27G4MFgyshY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract all known place name in a list\n",
        "def get_tagged_list(text, entities, tag='PLNAME'):\n",
        "  begin, tokens_tags = 0, []\n",
        "  for start, ent in entities.items():\n",
        "    if begin <= start:\n",
        "      tokens_tags.append((text[begin:start], None))\n",
        "      tokens_tags.append((text[start:start+len(ent)], tag))\n",
        "      begin = start+len(ent)\n",
        "  tokens_tags.append((text[begin:], None)) #add the last untagged chunk\n",
        "  return tokens_tags\n",
        "\n",
        "# get_tagged_list(example_text, extracted_place_names)"
      ],
      "metadata": {
        "id": "pwWUgDFLBKb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need inflections and lemmas of all the words in the geo nouns list. For example, if we have `road` in the list, then we will need `roads` as well, and vice versa.\n",
        "\n",
        "So we will install the `lemminflect` library and define the `get_inflections()` function."
      ],
      "metadata": {
        "id": "PGxmR_rGz1HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lemminflect"
      ],
      "metadata": {
        "id": "GsYdojryGGB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand list with inflections and lemmas\n",
        "from lemminflect import getLemma, getInflection\n",
        "def get_inflections(names_list):\n",
        "    gf_names_inflected = []\n",
        "    for w in names_list:\n",
        "      gf_names_inflected.append(w)\n",
        "      gf_names_inflected.extend(list(getInflection(w.strip(), tag='NNS', inflect_oov=False)))\n",
        "      gf_names_inflected.extend(list(getLemma(w.strip(), 'NOUN', lemmatize_oov=False)))\n",
        "    return list(set(gf_names_inflected))"
      ],
      "metadata": {
        "id": "4Y79jODFF6fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to define a background color dictionary `BG_COLOR` for visualization so that the system can decide what colour to apply in highliting the entities of different tags in the text. \n",
        "\n",
        "Accordingly, the entity backgound in the `mark_up()` will be modified to select a color from the dictionary using the tag as the key (i.e. `BG_COLOR[tag]`)."
      ],
      "metadata": {
        "id": "NN9hJrwV0-qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BG_COLOR = {'PLNAME':'#feca74','GEONOUN': '#9cc9cc'}\n",
        "# Marking up the token for visualization\n",
        "def mark_up(token, tag=None):\n",
        "  if tag:\n",
        "    begin_bkgr = f'<bgr class=\"entity\" style=\"background: {BG_COLOR[tag]}; padding: 0.1em 0.1em; margin: 0 0.15em; border-radius: 0.23em;\">'\n",
        "    end_bkgr = '\\n</bgr>'\n",
        "    begin_span = '<span style=\"font-size: 0.8em; font-weight: bold; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">'\n",
        "    end_span = '\\n</span>'\n",
        "    return f\"{begin_bkgr}{token}{begin_span}{tag}{end_span}{end_bkgr}\"\n",
        "  return f\"{token}\""
      ],
      "metadata": {
        "id": "1zsXUh4N4lOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we read the geo nouns saved in `geo_feature_nouns.txt` into a list and produce the tagged list with the extracted geo nouns using the `GEONOUN` tag. "
      ],
      "metadata": {
        "id": "hVnyZJmq2OUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read LD placenames into a list\n",
        "geonouns = get_inflections([noun.strip() for noun in open('geo_feature_nouns.txt').readlines()])\n",
        "tagged_geonouns = get_tagged_list(example_text, extract_entities(example_text, geonouns), 'GEONOUN')\n",
        "visualize(tagged_geonouns)"
      ],
      "metadata": {
        "id": "kw8m_Jm28USg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Extracting and visualising multiple entity types**"
      ],
      "metadata": {
        "id": "s2tF98RENrRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we can extract multiple entities from text, we have to modify or functions to be more generalizable. That way, we can pass any list of items of eny category that we are interested in.\n",
        "\n",
        "The `extract_entities()` function will also be rewritten to return not just the entities and their starting position but also their tags."
      ],
      "metadata": {
        "id": "l238Wj_4nqFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates a list of all tokens, tagged and untagged, for visualisation\n",
        "def extract_entities(text, ent_list, tag='PLNAME'):\n",
        "  sorted(set(ent_list), key=lambda x:len(x), reverse=True)\n",
        "  extracted_entities = {}\n",
        "  for ent in ent_list:\n",
        "    for match in re.finditer(f' {ent}[\\.,\\s\\n;:]', text):\n",
        "      # modified to return the `tag` too...\n",
        "      extracted_entities[match.start()+1]=text[match.start()+1:match.end()-1], tag\n",
        "  return {i:extracted_entities[i] for i in sorted(extracted_entities.keys())}\n",
        "\n",
        "# extract_entities(example_text, place_names)"
      ],
      "metadata": {
        "id": "dBfY8-TTnUqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, in building the tagged list of tokens for the visualizer, we need to include the tags of the entities (`PLNAME` for placenames and GEONOUN for geo feature nouns) and `None` for other tokens."
      ],
      "metadata": {
        "id": "GD__4kufOkVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates a list of all tokens, tagged and untagged, for visualisation\n",
        "def get_tagged_list(text, entities):\n",
        "  begin, tokens_tags = 0, []\n",
        "  for start, (ent, tag) in entities.items():\n",
        "    if begin <= start:\n",
        "      tokens_tags.append((text[begin:start], None))\n",
        "      tokens_tags.append((text[start:start+len(ent)], tag))\n",
        "      begin = start+len(ent)\n",
        "  tokens_tags.append((text[begin:], None)) #add the last untagged chunk\n",
        "  return tokens_tags\n",
        "\n",
        "# get_tagged_list(example_text, extracted_place_names)"
      ],
      "metadata": {
        "id": "txRLKNOhObPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need another function `merge_entities()` to combine the entities that we have extracted and their tags into a single dictionary."
      ],
      "metadata": {
        "id": "ZUtGZmlm9Wbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merging entities\n",
        "from collections import OrderedDict\n",
        "def merge_entities(first_ents, second_ents):\n",
        "  return OrderedDict(sorted({** second_ents, **first_ents}.items()))"
      ],
      "metadata": {
        "id": "PIv9ml6LJiOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try to extract, merge and visualize multiple entities (i.e. placenames and geo nouns).\n",
        "\n",
        "The code below will tag all extracted names as place names by default\n",
        "\n",
        "```python\n",
        "extracted_placenames = extract_entities(example_text, place_names)\n",
        "```\n",
        "\n",
        "However, in the code below, we have to explicitly pass `GEONOUN` to the tag parameter\n",
        "\n",
        "```python\n",
        "extracted_geonouns = extract_entities(example_text, geonouns, tag='GEONOUN')\n",
        "```"
      ],
      "metadata": {
        "id": "ldX8H-rH97MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_placenames = extract_entities(example_text, place_names)\n",
        "extracted_geonouns = extract_entities(example_text, geonouns, tag='GEONOUN')\n",
        "\n",
        "merged_entities = merge_entities(extracted_placenames, extracted_geonouns)"
      ],
      "metadata": {
        "id": "GldVd0K3M89b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of tagged entities (or placenames) and their tags\n",
        "tagged_list = get_tagged_list(example_text, merged_entities)\n",
        "visualize(tagged_list)"
      ],
      "metadata": {
        "id": "Mo3zV6uch05a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise:**\n",
        "\n",
        "The `locativeAdverbs.txt` file in the working folder contains a list of the locative adverbs (i.e *above*, *homewards*, *northbound*, *southwards* etc.\n",
        "\n",
        "**Task 1:** Use the code below to read the list into a Python variable `loc_advs`.\n",
        "\n",
        "```python\n",
        "loc_advs = [adv.split()[0] for adv in open('locativeAdverbs.txt').readlines()]\n",
        "```"
      ],
      "metadata": {
        "id": "Kgis8p2SXZ6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type code below...\n"
      ],
      "metadata": {
        "id": "evTgsugMkTBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:** Extract locative locative adverbs in the text using the following code.\n",
        "\n",
        "```python\n",
        "extracted_locadvs = extract_entities(example_text, loc_advs, tag='LOCADV')\n",
        "```"
      ],
      "metadata": {
        "id": "LfKDQsPrkhlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type code below...\n"
      ],
      "metadata": {
        "id": "03t21_I0kT5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3:** Modify the background colour dictionary `BG_COLOR` to add the colour for the `LOCADV` tag.\n",
        "\n",
        "```python\n",
        "BG_COLOR = {'PLNAME':'#feca74','GEONOUN': '#9cc9cc', 'LOCADV':'#f5b5cf'}\n",
        "```"
      ],
      "metadata": {
        "id": "2JLIU8ErkpV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type code below...\n"
      ],
      "metadata": {
        "id": "ZPm00FVLkTqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:** Visualize the locative adverbs in text with the `visualize()` function.\n",
        "\n",
        "```python\n",
        "visualize(get_tagged_list(example_text, extracted_locadvs))\n",
        "```"
      ],
      "metadata": {
        "id": "teqcWPOZkxCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type code below...\n"
      ],
      "metadata": {
        "id": "rSdIa-t7kTfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5:** Use `extract_entities()`, `merge_entities()` and `visualize()` functions to extract, merger and visualize **placenames**, **geo nouns** and **locative adverbs**."
      ],
      "metadata": {
        "id": "rbfhLfFJk3sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type code below...\n"
      ],
      "metadata": {
        "id": "wnuGtvKGkTSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Putting it all together..**"
      ],
      "metadata": {
        "id": "WifH5PM1EddJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the summary of the code that powers the rule-based extraction method in this notebook.\n",
        "\n",
        "```python\n",
        "!git clone https://github.com/IgnatiusEzeani/spatial_narratives_workshop.git\n",
        "```\n",
        "\n",
        "```python\n",
        "!pip install lemminflect\n",
        "```\n",
        "\n",
        "```python\n",
        "from IPython.display import HTML\n",
        "from collections import OrderedDict\n",
        "from lemminflect import getLemma, getInflection\n",
        "```\n",
        "\n",
        "```python\n",
        "BG_COLOR = {'PLNAME':'#feca74','GEONOUN': '#9cc9cc', 'LOCADV':'#f5b5cf'}\n",
        "```\n",
        "\n",
        "```python\n",
        "# Generates a list of all tokens, tagged and untagged, for visualisation\n",
        "def extract_entities(text, ent_list, tag='PLNAME'):\n",
        "  sorted(set(ent_list), key=lambda x:len(x), reverse=True)\n",
        "  extracted_entities = {}\n",
        "  for ent in ent_list:\n",
        "    for match in re.finditer(f' {ent}[\\.,\\s\\n;:]', text):\n",
        "      # modified to return the `tag` too...\n",
        "      extracted_entities[match.start()+1]=text[match.start()+1:match.end()-1], tag\n",
        "  return {i:extracted_entities[i] for i in sorted(extracted_entities.keys())}\n",
        "\n",
        "# Merging entities\n",
        "def merge_entities(first_ents, second_ents):\n",
        "  return OrderedDict(sorted({** second_ents, **first_ents}.items()))\n",
        "\n",
        "# Generates a list of all tokens, tagged and untagged, for visualisation\n",
        "def get_tagged_list(text, entities):\n",
        "  begin, tokens_tags = 0, []\n",
        "  for start, (ent, tag) in entities.items():\n",
        "    if begin <= start:\n",
        "      tokens_tags.append((text[begin:start], None))\n",
        "      tokens_tags.append((text[start:start+len(ent)], tag))\n",
        "      begin = start+len(ent)\n",
        "  tokens_tags.append((text[begin:], None)) #add the last untagged chunk\n",
        "  return tokens_tags\n",
        "\n",
        "# Marking up the token for visualization\n",
        "def mark_up(token, tag=None):\n",
        "  if tag:\n",
        "    begin_bkgr = f'<bgr class=\"entity\" style=\"background: {BG_COLOR[tag]}; padding: 0.05em 0.05em; margin: 0 0.15em;  border-radius: 0.55em;\">'\n",
        "    end_bkgr = '\\n</bgr>'\n",
        "    begin_span = '<span style=\"font-size: 0.8em; font-weight: bold; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">'\n",
        "    end_span = '\\n</span>'\n",
        "    return f\"{begin_bkgr}{token}{begin_span}{tag}{end_span}{end_bkgr}\"\n",
        "  return f\"{token}\"\n",
        "\n",
        "# generate html formatted text \n",
        "def visualize(token_tag_list):\n",
        "  start_div = f'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">'\n",
        "  end_div = '\\n</div>'\n",
        "  html = start_div\n",
        "  for token, tag in token_tag_list:\n",
        "    html += mark_up(token,tag)\n",
        "  html += end_div\n",
        "  return HTML(html)\n",
        "\n",
        "# Get inflections and lemmas of geo nouns\n",
        "def get_inflections(names_list):\n",
        "    gf_names_inflected = []\n",
        "    for w in names_list:\n",
        "      gf_names_inflected.append(w)\n",
        "      gf_names_inflected.extend(list(getInflection(w.strip(), tag='NNS', inflect_oov=False)))\n",
        "      gf_names_inflected.extend(list(getLemma(w.strip(), 'NOUN', lemmatize_oov=False)))\n",
        "    return list(set(gf_names_inflected))\n",
        "```"
      ],
      "metadata": {
        "id": "sgSSQ8ZxEvoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Next step...**\n"
      ],
      "metadata": {
        "id": "cXWNJZ4_l_pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition and Semantic Tagging**\n",
        "\n",
        "With the rule-based approach, we could extract the placenames, geo nouns, locative adverbs any other category of items in alist. \n",
        "\n",
        "However, it is limited in a number of ways.\n",
        "* It requires an exhaustive list of place names which is difficult to build for different types of writings.\n",
        "* Hand-crafted rules for all possible scenarios will need to be developed\n",
        "  - e.g. spelling errors, capitalizations, inflections etc.\n",
        "  - Over-lapping instances ('Eamont' vs 'Eamont Bridge')\n",
        "* It will be more difficult to extract references to time and date as well as sentiments and emotions.\n",
        "* The approach will not generalize well with other corpora\n",
        "\n",
        "In the next section, we will adapt some some existing tools - a named entity recognizer and a semantic tagger to try to mitigate some of the challenges above. "
      ],
      "metadata": {
        "id": "JFuaIOmSmBCa"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VQzZg_4jQ9Ch",
        "LCQBR8lJTAii",
        "jOdjjJwFHv11",
        "vb8f7aoPsYgL",
        "Zw9jVk97R4SB",
        "1tCywxkPi1r2",
        "WwolMngoq_hs"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}